# Paul Graham Essay Scraper

## Overview
This project is a web scraping tool designed to extract all essays written by Paul Graham from his website ([https://paulgraham.com](https://paulgraham.com)) using Python's Beautiful Soup library (`bs4`) and other supportive libraries. The extracted essays will be saved in a structured format, making them accessible for further analysis, reading, or processing.

## Features
- **Scraping Essays:** Extracts all essays from Paul Graham's website.
- **Content Storage:** Saves essays in a structured format (e.g., text files, JSON, or database).
- **Error Handling:** Handles common web scraping issues like missing pages, connection timeouts, and unexpected HTML structure.
- **Customizable Output:** Users can specify the format and location for saving the extracted essays.

## Requirements
- Python 3.7+
- Required libraries:
  - `requests`
  - `beautifulsoup4`
  - `pandas` 
  - `numpy` 

Install the dependencies with the following command:

```bash
pip install -r requirements.txt
```

## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/realaryagupta/Paul-Graham-Essay-Web-Scrapping.git
   cd paul-graham-scraper
   ```
2. Install the required Python packages:
   ```bash
   pip install -r requirements.txt
   ```

## Usage
1. Run the scraper script:
   ```bash
   python scraper.py
   ```

## File Structure
```
paul-graham-scraper/
â”œâ”€â”€ scraper.py         # Main script for web scraping
â”œâ”€â”€ requirements.txt   # Required Python libraries
```

## Implementation Details
### Scraping Logic
- **Homepage Parsing:** The scraper starts by accessing Paul Graham's homepage, which contains links to all essays.
- **Essay Extraction:** Each essay link is visited, and the content is extracted using `BeautifulSoup`.
- **Data Cleaning:** Removes unnecessary HTML tags and extracts the main content.
- **Storage:** Essays are saved in the desired format and structure.

### Error Handling
- **Retry Logic:** Retries failed requests a specified number of times.
- **Logging:** Logs errors and progress for debugging and tracking purposes.

## Customization
- **Output Format:** Modify the code to save essays in `.txt`, `.json`, or even export them to a database.
- **Filters:** Add filters to scrape specific essays based on keywords or topics.

## Future Enhancements
- Add support for:
  - Categorizing essays by topics
  - Sentiment analysis and keyword extraction
  - Generating a summarized version of each essay

## Contributing
Contributions are welcome! Feel free to open issues or submit pull requests to improve the project.

---

Happy Scraping! ðŸŽ‰
